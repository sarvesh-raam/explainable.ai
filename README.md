# ğŸ¤– Explainable AI (XAI) Framework
### *Balancing Accuracy and Interpretability: A Post-hoc Framework for Structured Data Analysis*

![Python](https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white)
![Scikit-Learn](https://img.shields.io/badge/Scikit--Learn-F7931E?style=for-the-badge&logo=scikit-learn&logoColor=white)
![SHAP](https://img.shields.io/badge/SHAP-Black?style=for-the-badge&logo=expert-systems&logoColor=white)
![XAI](https://img.shields.io/badge/Focus-Explainability-blue?style=for-the-badge)

---

## ğŸ¯ Project Overview
This research project investigates the reliability, stability, and utility of post-hoc explainability methods (**SHAP** and **LIME**) when applied to black-box machine learning models. We aim to bridge the gap between high-performance "black-box" models and the transparency required for critical decision-making in high-stakes environments.

### ğŸ§ª Central Research Question
> *"Can post-hoc explainability methods produce faithful, stable, and useful explanations for black-box modelsâ€”while maintaining high predictive performance across diverse structured datasets?"*

---

## ğŸ”¬ Research Objectives
*   **Empirical Comparison:** Evaluating the performance of Linear Models (Logistic Regression) vs. Ensemble Methods (Random Forest, Gradient Boosting).
*   **XAI Evaluation:** Comparing SHAP (Global & Local) vs. LIME (Local) explanations.
*   **Stability Analysis:** Measuring the robustness of explanations against data perturbations (Gaussian noise).
*   **Consistency Check:** Quantifying the agreement between different explanation methods to ensure trustworthy AI.

---

## ğŸ—ï¸ The 5-Phase Research Plan

| Phase | Title | Status | Description |
| :--- | :--- | :--- | :--- |
| **Phase 1** | **Dataset & Preprocessing** | âœ… Completed | Cleaning and engineering diverse structured datasets (e.g., UCI Repository). |
| **Phase 2** | **Model Training** | âœ… Completed | Building predictive engines using ensemble and linear methods. |
| **Phase 3** | **SHAP Analysis** | âœ… Completed | Global and local feature driver identification. |
| **Phase 4** | **LIME Analysis** | âœ… Completed | Local instance analysis and consistency comparisons. |
| **Phase 5** | **Stability Testing** | âœ… Completed | Quantifying the trustworthiness of explanations for publication. |

---

## ğŸ“ˆ Conclusion
This framework demonstrates that while advanced "black-box" models like Random Forest yield high predictive accuracy, **post-hoc explainability (SHAP & LIME)** provides the necessary transparency for high-stakes decision-making. Our **Stability Testing** confirms the robustness of these explanations, ensuring that small perturbations in data do not lead to wildly inconsistent interpretationsâ€”a critical requirement for trust in AI systems.


## ğŸ› ï¸ Tech Stack
*   **Core Logic:** `Python 3.x`
*   **Data Handling:** `pandas`, `NumPy`
*   **Machine Learning:** `scikit-learn`, `XGBoost`
*   **Explainability:** `SHAP`, `LIME`
*   **Visualization:** `Matplotlib`, `Seaborn`

---

## ğŸ“‚ Directory Structure
```bash
Explainable-AI/
â”œâ”€â”€ data/               # Raw and processed datasets
â”œâ”€â”€ src/                # Python source code for preprocessing & modeling
â”œâ”€â”€ notebooks/          # Experimental analysis and Jupyter Visualizations
â”œâ”€â”€ results/            # Research plots, tables, and findings
â””â”€â”€ requirements.txt    # Project dependencies
```

---

## ğŸ¤ Collaborators

| <a href="https://github.com/sarvesh-raam"><img src="https://github.com/sarvesh-raam.png" width="120px;" style="border-radius: 50%;" alt=""/></a> | <a href="https://github.com/Vigneshhhhhhhhhh"><img src="https://github.com/Vigneshhhhhhhhhh.png" width="120px;" style="border-radius: 50%;" alt=""/></a> |
| :---: | :---: |
| **[sarvesh-raam](https://github.com/sarvesh-raam)** | **[Vigneshhhhhhhhhh](https://github.com/Vigneshhhhhhhhhh)** |

---

## ğŸ“œ License
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
